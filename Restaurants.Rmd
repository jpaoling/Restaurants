---
output: 
  html_notebook:
    toc: TRUE
    toc_float: TRUE
title: "Restaurant Inspections"
---

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(readxl)
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction

In this data analysis project, I will use data on restaurant inspections to predict the number of days until the next inspection occurs, for a given restaurant.

The data set contains information on restaurant inspections in New York City carried out by the Department of Health and Hygiene (DOHMH) between 2010 and 2017. It contains the following variables:

* CAMIS: a unique identifier for the restaurant

* DBA: name of (doing business as) the restaurant

* BORO: borough in which the restaurant is located

* BUILDING: building number for the restaurant

* STREET: street name at which the restaurant is located

* ZIPCODE: Zip code as per the (mailing) address of the restaurant

* PHONE: phone number

* CUISINE DESCRIPTION: cuisine of the restaurant

* INSPECTION DATE: date of inspection

* ACTION: action associated with the given inspection

* VIOLATION CODE: violation code associated with the given inspection

* VIOLATION DESCRIPTION: description that corresponds to violation code

* CRITICAL FLAG: indication if violation is critical or not

* SCORE: total score for a particular inspection

* GRADE: grade issued for the given inspection

* GRADE DATE: date when the current grade was issued to the restaurant

* RECORD DATE: The date when the extract was run to produce this data set 

* INSPECTION TYPE: The type of inspection. A combination of the program and inspection type

We read in the data:

```{r warning=FALSE}
(restaurants_raw <- read_excel("New_York_City_Restaurants.xlsx"))
```

For convenience, we rename the variables. 

```{r echo=FALSE}
new_names <- c("id", "rest_name", "boro", "building", "street", "zipcode", 
               "phone", "cuisine_descr", "inspection_date", "action", 
               "violation_code", "violation_descr", "critical_flag", 
               "score", "grade", "grade_date", "record_date", 
               "inspection_type")
(names(restaurants_raw) <- new_names)
rm(new_names)
```


## 2. Exploratory Analysis

Let us turn to the exploration of the data..

### 2.1 Inspection Date

```{r}
restaurants_raw$inspection_date <- ymd(restaurants_raw$inspection_date) 
summary(restaurants_raw$inspection_date)
```

Apparently, there are some observations with an inspection date of "1900-01-01". Of course, these are wrong values. On closer inspection we see that values across all the variables relating to grading are missing for those observations

```{r echo=FALSE}
restaurants_raw %>% 
  summarise(na_inspectionDate = round((sum(inspection_date == "1900-01-01")/n()), 4))
```

Those obervations amount to .03 % of all observations. For convenience, we will ignore them. 

```{r echo=FALSE}
restaurants_raw <- restaurants_raw %>% 
  filter(!inspection_date == "1900-01-01")
```



### 2.2 Cuisine description

```{r}
restaurants_raw$cuisine_descr <- as.factor(restaurants_raw$cuisine_descr)

# Relabel level "CafÃƒÂ©/Coffee/Tea" which is at position 14 in levels vector
cuis_ind <- levels(restaurants_raw$cuisine_descr) == levels(restaurants_raw$cuisine_descr)[14]
levels(restaurants_raw$cuisine_descr)[cuis_ind] <- "Coffee_Tea"

# Relabel level "Latin (..." to "Latin"
levels(restaurants_raw$cuisine_descr)[levels(restaurants_raw$cuisine_descr) == "Latin (Cuban, Dominican, Puerto Rican, South & Central American)"] <- "Latin"

# Lump together the least common categories, keeping the 20 most frequent.
restaurants_raw <- restaurants_raw %>% 
  mutate(cuisine_descr = fct_lump(f = cuisine_descr, n = 20))
```

Let us look at the distribution of 'cuisine_descr':

```{r}
restaurants_raw %>% 
  group_by(cuisine_descr) %>% 
  summarise(n = n_distinct(id)) %>% 
  ungroup() %>% 
  ggplot(aes(x = reorder(cuisine_descr, -n), y = n)) +
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  xlab("Cuisine")
```

The most frequent cuisines are those labelled 'American', 'Other' and 'Chinese'. 


### 2.3 Boros

Next, let us take a look at the distribution of 'Boro'.

```{r}
restaurants_raw$boro <- as.factor(restaurants_raw$boro)
levels(restaurants_raw$boro)[levels(restaurants_raw$boro) == "Missing"] <- NA
summary(restaurants_raw$boro)
```

```{r}
restaurants_raw %>% 
  group_by(boro) %>% 
  summarize(n = n_distinct(id)) %>% 
  ggplot(aes(x = reorder(boro, -n), y = n)) +
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle=60, hjust=1))
```

Most of the eating establishments are located in Manhattan, followed by Brooklyn and Queens.


### 2.4 Actions taken

Next, we look at the variable 'actions taken'. The variable takes on the following values:

```{r echo=FALSE}
levels(as.factor(restaurants_raw$action))
```

We convert it into a factor and (re)name its levels for convenience:

```{r}
restaurants_raw$action <- as.factor(restaurants_raw$action)
levels(restaurants_raw$action) <- c("Closed", "ReClosed", "ReOpened", "NoViol", "YesViol")
summary(restaurants_raw$action)
```

Now we can see what the most frequent categories are:


```{r echo=FALSE}
(action_df <- restaurants_raw %>% 
  group_by(action) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n)))

action_df %>% 
  ggplot(aes(reorder(action, -n), n)) +
  geom_bar(stat = "identity") +
  xlab("Action taken by DOHMH") 
```

The most frequent category is 'YesViol'.  


### 2.5 Violation types

Next, we will consider the different types of violations recorded. The most common violation codes are listed below:

```{r}
restaurants_raw$violation_code <- as.factor(restaurants_raw$violation_code)
restaurants_raw %>% 
  group_by(violation_code) %>% 
  summarise(number = n()) %>% 
  mutate(rank = rank(desc(number))) %>% 
  filter(rank <= 10) %>% 
  arrange(rank)
```

10F (general violation pertaining to non-food contact surfaces) is the most common violation type. followed by 08A (facility not vermin proof), 04L (Evidence of mice).

Here is a quick plot of the distribution:

```{r}
restaurants_raw %>% 
  ggplot(aes(x = violation_code)) +
  geom_bar() +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  xlab("Violation code")
```

To get a better sense of what is going on, we group the values in the range of violation_code into broader categories. To do this, we use information from the provided by the authorities. Accordingly, violations are either scored or unscored. Among scored violations, we have to distinguish between critical and general violations.

```{r}
# New variable violation_group:
restaurants_raw <- 
  restaurants_raw %>% 
  mutate(violation_group = case_when(
    violation_code %in% str_c("02", LETTERS[1:10]) ~ "food_temperature",
    violation_code %in% c(str_c("03", LETTERS[1:7]), str_c("09", LETTERS[1:3])) ~ "food_source",
    violation_code %in% str_c("04", LETTERS[1:10]) ~ "food_protection",
    violation_code %in% c(str_c("05", LETTERS[1:9]), str_c("10", LETTERS[1:10])) ~ "facility",
    violation_code %in% str_c("06", LETTERS[1:9]) ~ "hygiene",
    violation_code %in% str_c("04", LETTERS[11:15]) ~ "vermin",
    violation_code %in% c("07A", "99B") ~ "other_scored",
    !is.na(violation_code) ~ "not_scored"
  )
)
```

Here is a bar plot displaying the distribution
```{r}
restaurants_raw %>% 
  group_by(violation_group) %>% 
  filter(!is.na(violation_group)) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(reorder(violation_group, -n), n)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle=60, hjust=1))
```

Some categories barely contain any observations. For the time being, we will keep the chosen binning. We are going to revisit this point in the context of feature engineering.

Here is the distribution of violation code again. This time we incorporate the variable violation group.

```{r}
restaurants_raw %>% 
  filter(!is.na(violation_group)) %>% 
  ggplot(aes(x = violation_code, fill = violation_group)) +
  geom_bar() +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

For later use, we construct indicator variables for the different violation groups:

```{r}
for(level in unique(restaurants_raw$violation_group)){
  if (is.na(level)) {
    next
  } else {
    restaurants_raw[paste("viol", level, sep = "_")] <- 
    ifelse(restaurants_raw$violation_group == level, 1, 0)
  }
}
names(restaurants_raw)
```



### 2.6 Inspection types

The variable inspection type takes on 34 distinct values:

```{r}
restaurants_raw %>% 
     summarise(n_distinct(inspection_type))
```

These are the values:

```{r echo=FALSE}
restaurants_raw %>% 
  group_by(inspection_type) %>% 
  summarise()
```

We collapse these values into a smaller range:

```{r echo=FALSE}
restaurants_raw$inspection_type2 <-  restaurants_raw %>% 
  select(inspection_type) %>% 
  separate(inspection_type, into = c("before_slash", "after_slash"), sep = " / ") %>% 
  transmute(after_slash = str_replace_all(after_slash, fixed(" "), fixed(""))) %>% 
  transmute(inspection_type2 = str_replace_all(after_slash, fixed("-i"), fixed("I"))) %>% 
  .$inspection_type2

unique(restaurants_raw$inspection_type2)
```

Here is a display of the distribution:

```{r echo=FALSE}
restaurants_raw %>% 
  group_by(inspection_type2) %>% 
  summarise(n = n()) %>% 
  ggplot(aes(reorder(as.factor(inspection_type2), -n), n)) +
  geom_bar(stat = "identity", color = "orange") +
  xlab("Inspection type") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Again, there a are categories that are 'almost empty'. The vast majority of observations are initial and re-inspections:

```{r echo=FALSE}
restaurants_raw %>% 
  group_by(inspection_type2) %>% 
  summarise(n = n(), prop = round(n/nrow(restaurants_raw), 5)) %>% 
  arrange(desc(n))
```

For later purposes, we construct a dummy variable for the category 'InitialInspection'.

```{r echo=FALSE}
restaurants_raw$dummy_InitialInspection <- 
  as.factor(ifelse(restaurants_raw$inspection_type2 == "InitialInspection", "Initial", "Not_Initial"))
names(restaurants_raw)
```


### 2.7 Grade

Now on to 'grade'.

```{r echo=FALSE}
restaurants_raw %>% 
  group_by(grade) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n))
```

There is a massive amount of NA's. We are going to investigate how the number of NAs for ‘grade’ relates to ‘score’ and ‘inspection_type.’ More precisely, we know that on initial inspections no grade is issued, if the corresponding score is above 14. It seems plausible that this would lead to a value of NA for grade.

```{r}
restaurants_raw %>% 
  select(inspection_type2, score, grade) %>% 
  filter(inspection_type2 == "InitialInspection", score > 14) %>%
  group_by(grade) %>% 
  summarise(n = n())
```

So about 73 % of the missing values are due to a score of 14 or higher. Furthermore, out of the 1915 observations with value ‘Not yet graded’, 1548 are the result of a high score (above 13) in the initial inspection. We will recode these NA’s to “Not Yet Graded” to reduce the number of missing values for ‘grade’. This reduces the number of NA's significantly:

```{r echo=FALSE}
# Recode NA's with inspection type = 'InitialInspection' and 'score' > 14 to "Not Yet Graded":
restaurants_raw <- restaurants_raw %>%
  mutate(grade = case_when(
    is.na(.$grade) &
      .$inspection_type2 == "InitialInspection" &
      .$score > 14 ~ "Not Yet Graded",
    TRUE ~ grade
  ))
```


```{r}
restaurants_raw %>% 
  summarise(prop_na = round(mean(is.na(grade)), 2),
            n_not_graded = sum(grade == "Not Yet Graded", na.rm = T))
```

Here is the distribution of the proportion of inspections with an NA for grade per restaurant:

```{r echo=FALSE}
restaurants_raw %>% 
  group_by(id, inspection_date) %>% 
  summarise(prop_gradeNA = mean(is.na(grade))) %>% 
  ggplot(aes(x = prop_gradeNA)) + 
  geom_histogram(bins = 30) + 
  xlab("proportion of NA's for inspection grade per restaurant")
```

It seems that for most restaurants there is no inspection with a missing value for grade. On the other hand, there are some restaurants for whom all values for grade are NA.

```{r echo=FALSE}
restaurants_raw %>% 
  group_by(id, inspection_date) %>% 
  summarise(prop_gradeNA = round(mean(is.na(grade)), 3)) %>% 
  ungroup() %>% 
  group_by(prop_gradeNA) %>% 
  summarise(proportion_of_restaurants = round(n()/nrow(.), 3)) %>% 
  arrange(desc(proportion_of_restaurants))
```


It would be worth to further investigate the nature of the NA's related grade, something which will not pursue any further at this point. For later purposes, we convert 'grade' into a factor:

```{r}
restaurants_raw <- restaurants_raw %>% 
  mutate(grade = as.factor(grade))
```



### 2.8 Score

Let us turn to 'score'.

```{r}
summary(restaurants_raw$score)
```



There are 19514 NA’s. So let us look at the proportion of inspections that exhibit an NA for 'score'. 

```{r echo=FALSE}
restaurants_raw %>% 
  group_by(id, inspection_date) %>% 
  summarise(n_na = sum(is.na(score))) %>% 
  ungroup() %>% 
  summarise(n_na = round(sum(n_na)/n(), 3))
```

15 % of all inspections have a missing value for 'score'.


We know that there are 16530 observations that refer to violations that are not scored:

```{r echo=FALSE}
restaurants_raw %>% 
  group_by(violation_group) %>% 
  summarise(n = n()) %>% 
  filter(violation_group == "not_scored")    
```


These observations are expected to have NA for ‘score’:

```{r echo=FALSE}
restaurants_raw %>% 
  select(violation_group, score) %>% 
  filter(violation_group == "not_scored") %>% 
  summarise(n = n(), n_na_score = sum(is.na(score)), prop_na_score = round(n_na_score/n, 2))
```

Hence, these observations account for 82 % of all of the 19514 NA’s for ‘score’. What about the remaining 3428 missing values for ‘score’? Let us consider the relationship with violation group:

```{r echo=FALSE}
restaurants_raw %>%
  select(violation_group, score) %>%
  filter(is.na(violation_group)) %>%
  summarise(n = n(), n_na_score = sum(is.na(score)))
```

Hence, the rest of the NA’s for score are associated with NA’s on ‘violation_group’. So let us turn to the NA’s for the variable ‘violation_group’. One cause for an NA here may be that on a given inspection, there was no violation at all. So what is the number of NA's for violation group among observations that do not exhibit a violation?

```{r echo=FALSE}
restaurants_raw %>% filter(action == "NoViol") %>% 
  summarise(viol_group_NA = sum(is.na(violation_group)))
```



## 3. Predicting time until the next inspection

Finally, we now return to the question posed in the beginning, namely, whether it is possible to predict the number of days until the next inspection, using features of the data, described above. 


### 3.1 Split into training and test data

As a first step, we will divide our data set into a training and a test data set, putting 2/3 of the restaurants into the training set.

```{r}
set.seed(1)
train_ids <- sample(unique(restaurants_raw$id), 0.6*length(unique(restaurants_raw$id)))
test_ids <- setdiff(unique(restaurants_raw$id), train_ids) 
```

Next, let us see if the distributions of the target feature in training and test data are similar:

```{r echo=FALSE}
restaurants_raw %>% 
  select(id, inspection_date) %>% 
  group_by(id, inspection_date) %>% 
  arrange(inspection_date) %>% 
  summarise() %>% 
  mutate(difference = lead(inspection_date) - inspection_date) %>%
  filter(!is.na(difference)) %>% 
  mutate(group = ifelse(id %in% train_ids, "Training", "Test")) %>% 
  ggplot(aes(as.numeric(difference))) +
  geom_histogram(bins = 30) +
  scale_x_continuous(breaks = seq(0, 1000, by = 100)) +
  xlab("Days until next inspection") +
  facet_grid(. ~ group) 
```

The distribution of the target seems reasonably similar across training and test data. 



### 3.2 Features

In the following we will use regression trees to predict the number of days until the next inspection, using the score and grade on the last inspection, the type of cuisine and number of violations in the different violation groups on the last inspection. Furthermore, we will impute missing values for score and grade using regression trees, including only those observations for which the respective values are not missing. 

```{r echo=FALSE}
## Create data set with features for inspection type, score, grade and number of violations:
make_feature_set <- function ( df_raw, is_train ) {
  
  make_df_GradeInspecScoreCuisine <- function ( df ) {
    
    df %>% 
      select ( id, inspection_date, dummy_InitialInspection, score, 
               grade, cuisine_descr ) %>% 
      group_by( id, inspection_date ) %>% 
      arrange ( id, inspection_date ) %>% 
      summarise_at ( vars(c("grade","dummy_InitialInspection", "score", "cuisine_descr")), 
                     .funs = list(first) ) 

  }
  
  make_df_sumViolFlags <- function(df) {
    
    df %>% 
      select ( id, inspection_date, starts_with("viol_"), starts_with("critical") ) %>% 
      mutate( critical_flag = case_when( critical_flag == "Critical" ~ 1, 
                                         TRUE ~ 0 ) ) %>% 
      group_by ( id, inspection_date ) %>% 
      arrange ( id, inspection_date ) %>% 
      summarise_at ( vars(starts_with( "viol" ), starts_with( "critical" ) ), ~ sum( ., na.rm = TRUE ) ) 

  }
  
  make_df <- function(df) {
    
    left_join ( make_df_GradeInspecScoreCuisine ( df ) , make_df_sumViolFlags ( df ) ,
               by = c ( "id", "inspection_date" ) )

  }
  
  # Data set with features and response
  if ( is_train ) {
    
    df_features <- make_df ( df_raw ) %>% 
      mutate ( days_until_next = lead ( inspection_date ) - inspection_date ) %>% 
      filter ( !is.na ( days_until_next ) ) %>% 
      ungroup() %>% 
      mutate ( days_until_next_categ = case_when (
        between ( as.numeric ( days_until_next ), 0, 100 ) ~ "within 2 to 3 months",
        between ( as.numeric ( days_until_next ), 101, 300 ) ~ "within 10 months",
        TRUE ~ "in more than 10 months" ),
        days_until_next_categ = as.factor ( days_until_next_categ ) ) 
    
    
  } else {
    
    df_features <- make_df ( df_raw ) %>% ungroup()
    
  }
  
  df_features
  
}

# Impute missing values for score and grade:
impute_features <- function ( df_features ) {
  
  scoreFit <- df_features %>%
    filter ( !is.na ( score ) ) %>%
    select ( score, grade, dummy_InitialInspection, cuisine_descr,
           starts_with ( "viol_" ), critical_flag ) %>%
    rpart ( score ~ ., data = ., method = "anova" )
  df_features$score[is.na( df_features$score )] <-
    predict(scoreFit, df_features[is.na(df_features$score), ])
  
  gradeFit <- df_features %>%
    filter(!is.na(grade)) %>%
    select(score, grade, dummy_InitialInspection, cuisine_descr,
           starts_with("viol_"), critical_flag) %>%
    rpart(grade ~ ., data = ., method = "class")
  df_features$grade[is.na(df_features$grade)] <-
    predict(gradeFit, df_features[is.na(df_features$grade), ], type = "class")
  
  
  df_features
  
}
```


```{r}
library(rpart)
df_features <- make_feature_set(restaurants_raw, is_train = TRUE) %>% impute_features()

df_features
```

```{r}
df_train <- df_features %>% filter(id %in% train_ids)
df_test <- df_features %>% filter(id %in% test_ids)
```


### 3.3 Feature Exploration

Let us first explore a couple of bivariate relationships between the predictors and the target feature. We will begin with the relationship between grade and days until the next inspection

```{r echo=FALSE}
df_train %>%
  ggplot ( aes ( x = grade, y = as.numeric ( days_until_next ) ) ) +
  geom_boxplot ( fill = "grey") + 
  geom_jitter(alpha = .1, width = .07) +
  xlab("grade") +
  ylab("days until next inspection")
```

As we can see, the median number of days until the next inspection decreases when moving from A to C. This is not suprising, since a good grade on the initial inspection reduces the likelihood of being subjected to a re-inspection the same inspection cycle.

Let us move on to the relationship between violation types and the target feature:

```{r echo=FALSE}
g_vermin <- df_train %>% 
  ggplot(aes(x = as.factor(viol_vermin), y = as.numeric(days_until_next))) + 
  geom_boxplot() +
  xlab("Number of violations") +
  ylab("Days until next inspection") +
  ggtitle("Vermin")

g_not_scored <- df_train %>% 
  ggplot(aes(x = as.factor(viol_not_scored), y = as.numeric(days_until_next))) + 
  geom_boxplot() +
  xlab("Number of violations") +
  ylab("Days until next inspection") +
  ggtitle("Not scored") 

g_facility <- df_train %>% 
  ggplot(aes(x = as.factor(viol_facility), y = as.numeric(days_until_next))) + 
  geom_boxplot() +
  xlab("Number of violations") +
  ylab("Days until next inspection") +
  ggtitle("Facility")

g_food_temperature <- df_train %>% 
  ggplot(aes(x = as.factor(viol_food_temperature), y = as.numeric(days_until_next))) + 
  geom_boxplot() +
  xlab("Number of violations") +
  ylab("Days until next inspection") +
  ggtitle("Food temperature")

g_hygiene <- df_train %>% 
  ggplot(aes(x = as.factor(viol_hygiene), y = as.numeric(days_until_next))) + 
  geom_boxplot() +
  xlab("Number of violations") +
  ylab("Days until next inspection") +
  ggtitle("Hygiene")

g_food_protection <- df_train %>% 
  ggplot(aes(x = as.factor(viol_food_protection), y = as.numeric(days_until_next))) + 
  geom_boxplot() +
  xlab("Number of violations") +
  ylab("Days until next inspection") +
  ggtitle("Food protection")

g_food_source <- df_train %>% 
  ggplot(aes(x = as.factor(viol_food_source), y = as.numeric(days_until_next))) + 
  geom_boxplot() +
  xlab("Number of violations") +
  ylab("Days until next inspection") +
  ggtitle("Food source")

g_other_scored <- df_train %>% 
  ggplot(aes(x = as.factor(viol_other_scored), y = as.numeric(days_until_next))) + 
  geom_boxplot() +
  xlab("Number of violations") +
  ylab("Days until next inspection") +
  ggtitle("Other scored")

library(gridExtra)
grid.arrange(g_vermin, g_facility, 
             g_food_protection, g_food_source, g_food_temperature, 
             g_hygiene, g_not_scored, g_other_scored, ncol = 3)
```

Here also, as expected, the median number of days until the next inspection decreases as the number of violations per inspection increases.

### 3.4 Linear Regression model

We start with a simple linear regression model, using all features.

```{r echo=FALSE}
library(caret)
(lin_reg_model <- df_train %>% 
    select(-days_until_next_categ, -id, -inspection_date) %>%
    train(as.numeric(days_until_next) ~ .,
          data = .,
          method = "lm"))
```

We see that the R squared associated with the model is 0.476, i.e., 47.6 percent of the variation in the data can be 'explained' in terms of the linear regression model. Let us look at the residual plot:

```{r echo=FALSE}
df_lin_reg <- data_frame(actual = as.numeric(df_train$days_until_next), 
                         predicted = as.numeric(predict(lin_reg_model, df_train)), 
                         residual = actual - predicted)

df_lin_reg %>% 
  ggplot(aes(x = predicted, y = residual)) +
  geom_point() +
  geom_smooth()
```

There are a few curious things to notice about the plot. First, apparently there is a set of restaurants for which the model predicts a negative duration until the next inspection, which does not make sense. Second, there seems to be some pattern in the residuals. In the presence of nonlinearities, the conclusions drawn from the fit are questionable. At this point, one could investigate how these patterns in the residuals come about. Instead, in the next section, we will fit a nonlinear regression model to the data. 


### 3.5 Second Model: Regression tree

So far, so good. Let us fit at a cross-validated regression tree model, using all the features. First, we set the hyperparameters:

```{r}
# Set hyperparameters for cross-validated classification and regression tree
set.seed(42)
myControl <- trainControl(
  
  method = "cv",
  number = 10
  
)
cp.grid <- expand.grid(.cp = (1:10)*0.01)
```

Then we fit the model, employing 10-fold cross validation:

```{r echo=FALSE}
set.seed(3333)
(reg_tree_model <- df_train %>% select(-days_until_next_categ, -id, -inspection_date) %>% 
    train(as.numeric(days_until_next) ~ .,
          data = .,
          method = "rpart",
          trControl = myControl,
          tuneGrid = cp.grid))
library(rpart.plot)
rpart.plot ( reg_tree_model$finalModel, type = 3, digits = 3, fallen.leaves = TRUE )
```

The model has a slightly higher R squared than the linear regression model. Nevertheless, the fit of the regression to the training data is quite poor.


### 3.6 Reformulating the problem: Classification


As we have seen, linear regression and regression trees did a poor job of predicting the number of days until the next inspection. This is disappointing, but we can rephrase the problem. Instead of predicting the number of days, we could give a time interval into which the next inspection will fall.  



### 3.7 Classification model

As a next step, we will turn the continuous target into a categorical variable, by dividing the range into bins. The first bin will be the 0 - 100 days, the second 101 - 300 and the third bin more than 300 days. Then we will fit a classification tree to the data, using 10-fold cross validation:

```{r}
set.seed(3333)
(class_tree_model <- df_train %>% select(-days_until_next, -id, -inspection_date) %>% 
    train(days_until_next_categ ~ ., 
          data = ., 
          method = "rpart", 
          trControl = myControl, 
          tuneGrid = cp.grid)
)
rpart.plot ( class_tree_model$finalModel, type = 3, digits = 3, fallen.leaves = TRUE )
```

As we can see, the accuracy of the model is 0.819. Let us look at additional measures of performance:

```{r echo=FALSE}
confusionMatrix(data = predict(class_tree_model, df_train),
                reference = df_train$days_until_next_categ)
```

Sensitivity and specificity of all models are at least 0.84 for all classes, except for the third class, for which sensitiviy is at 0.71. The model 

### 3.8 Evaluation of final model

Finally we evaluate the final model on the test data:

```{r echo=FALSE}
confusionMatrix(data = predict(class_tree_model, df_test),
                reference = df_test$days_until_next_categ)
```

Hence, the performance of the model on the test data is similar than on the training data.

### 3.9 Conclusion

Our initial goal to predict the number of days until the next inspection could not be attained. Instead, we rephrased the problem and turned it into a classification problem. Then, we fit a classification tree on the training data, employing cross validation. The resulting model also performed well on the test data. So eventually, we have provided our client with a device that predicts a time window the next inspection is likely to occur in. 