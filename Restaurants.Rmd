---
output: 
  html_notebook:
    toc: TRUE
    toc_float: TRUE
title: "Restaurant Inspections"
---

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(readxl)
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction

In this data analysis project, I will try to use data on restaurant inspections to predict the number of days until the next inspection.

The data set I use contains information on restaurant inspections in New York City carried out by the Department of Health and Hygiene (DOHMH) between 2010 and 2017. The data set contains the following variables:

* CAMIS: a unique identifier for the restaurant

* DBA: name of (doing business as) the restaurant

* BORO: borough in which the restaurant is located

* BUILDING: building number for the restaurant

* STREET: street name at which the restaurant is located

* ZIPCODE: Zip code as per the (mailing) address of the restaurant

* PHONE: phone number

* CUISINE DESCRIPTION: cuisine of the restaurant

* INSPECTION DATE: date of inspection

* ACTION: action associated with the given inspection

* VIOLATION CODE: violation code associated with the given inspection

* VIOLATION DESCRIPTION: description that corresponds to violation code

* CRITICAL FLAG: indication if violation is critical or not

* SCORE: total score for a particular inspection

* GRADE: grade issued for the given inspection

* GRADE DATE: date when the current grade was issued to the restaurant

* RECORD DATE: The date when the extract was run to produce this data set 

* INSPECTION TYPE: The type of inspection. A combination of the program and inspection type

We read in the data:

```{r warning=FALSE}
(restaurants_raw <- read_excel("New_York_City_Restaurants.xlsx"))
```

For convenience, we rename the variables. 

```{r}
new_names <- c("id", "rest_name", "boro", "building", "street", "zipcode", 
               "phone", "cuisine_descr", "inspection_date", "action", 
               "violation_code", "violation_descr", "critical_flag", 
               "score", "grade", "grade_date", "record_date", 
               "inspection_type")
(names(restaurants_raw) <- new_names)
rm(new_names)
```


## 2. Exploratory Analysis

Let us turn to the exploration of the data..

### 2.1 Inspection Date

```{r}
restaurants_raw$inspection_date <- ymd(restaurants_raw$inspection_date) 
summary(restaurants_raw$inspection_date)
```

Apparently, there are some obeservations with an inspection date of "1900-01-01". Of course, these are wrong values. Let us look at these observations, in particular at the variables that relate to the grade of an inspection.

```{r}
restaurants_raw %>% 
  filter(inspection_date == "1900-01-01") %>% 
  select(action:inspection_type)
```

Values of all observations across all the variables relating to grading are missing. 

```{r}
restaurants_raw %>% 
  summarise(na_inspectionDate = round((sum(inspection_date == "1900-01-01")/n()), 4))
```

Those obervations amount to .03 % of all observations. For now, we will ignore those observations. 

```{r}
restaurants_raw <- restaurants_raw %>% 
  filter(!inspection_date == "1900-01-01")
```


(Furthermore, we will restrict our analysis to inspections that occured before 2017:)

```{r}
# restaurants_raw <- restaurants_raw %>%
  # filter("2011-06-01" <= inspection_date, inspection_date < "2017-02-03")
```

### 2.2 Cuisine description

```{r}
restaurants_raw$cuisine_descr <- as.factor(restaurants_raw$cuisine_descr)

# Relabel level "CafÃƒÂ©/Coffee/Tea" which is at position 14 in levels vector
cuis_ind <- levels(restaurants_raw$cuisine_descr) == levels(restaurants_raw$cuisine_descr)[14]
levels(restaurants_raw$cuisine_descr)[cuis_ind] <- "Coffee_Tea"

# Relabel level "Latin (..." into "Latin"
levels(restaurants_raw$cuisine_descr)[levels(restaurants_raw$cuisine_descr) == "Latin (Cuban, Dominican, Puerto Rican, South & Central American)"] <- "Latin"

# Lump together
restaurants_raw <- restaurants_raw %>% 
  mutate(cuisine_descr = fct_lump(f = cuisine_descr, n = 20))
```

Let us look at the distribution of 'cuisine_descr':

```{r}
restaurants_raw %>% 
  group_by(cuisine_descr) %>% 
  summarise(n = n_distinct(id)) %>% 
  ungroup() %>% 
  ggplot(aes(x = reorder(cuisine_descr, -n), y = n)) +
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle=60, hjust=1))
```



### 2.3 Boros

Next, let us take a look at the distribution of 'Boro'.

```{r}
restaurants_raw$boro <- as.factor(restaurants_raw$boro)
levels(restaurants_raw$boro)[levels(restaurants_raw$boro) == "Missing"] <- NA
summary(restaurants_raw$boro)
```

```{r}
restaurants_raw %>% 
  group_by(boro) %>% 
  summarize(n = n_distinct(id)) %>% 
  ggplot(aes(x = reorder(boro, -n), y = n)) +
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle=60, hjust=1))
```

Most observations are from Manhattan.


### 2.4 Actions taken

```{r}
levels(as.factor(restaurants_raw$action))
```

We convert it into a factor and (re)name its levels for convenience:

```{r}
restaurants_raw$action <- as.factor(restaurants_raw$action)
levels(restaurants_raw$action) <- c("Closed", "ReClosed", "ReOpened", "NoViol", "YesViol")
summary(restaurants_raw$action)
```



```{r}
(action_df <- restaurants_raw %>% 
  group_by(action) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n)))

action_df %>% 
  ggplot(aes(reorder(action, -n), n)) +
  geom_bar(stat = "identity") +
  xlab("Action taken by DOHMH") 
```

In the vast majority of inspections violations were cited. Among these, a very small number leads to restaurant closures. 


### 2.5 Violation types

Next, we will consider the different types of violations recorded.

```{r}
restaurants_raw$violation_code <- as.factor(restaurants_raw$violation_code)

restaurants_raw %>% 
  group_by(violation_code) %>% 
  summarise(number = n()) %>% 
  mutate(rank = rank(desc(number))) %>% 
  filter(rank <= 10) %>% 
  arrange(rank)
```

10F (general violation pertaining to non-food contact surfaces) is the most common violation type. followed by 08A (facility not vermin proof), 04L (Evidence of mice).

Here is a quick plot of the distribution:

```{r}
restaurants_raw %>% 
  ggplot(aes(x = violation_code)) +
  geom_bar() +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  xlab("Violation code")
```

To get a better sense of what is going on, we group the values in the range of violation_code into broader categories. To do this, we use information from the provided by the authorities. Accordingly, violations are either scored or unscored. Among scored violations, we have to distinguish between critical and general violations.

```{r}
# New variable violation_group:
restaurants_raw <- 
  restaurants_raw %>% 
  mutate(violation_group = case_when(
    violation_code %in% str_c("02", LETTERS[1:10]) ~ "food_temperature",
    violation_code %in% c(str_c("03", LETTERS[1:7]), str_c("09", LETTERS[1:3])) ~ "food_source",
    violation_code %in% str_c("04", LETTERS[1:10]) ~ "food_protection",
    violation_code %in% c(str_c("05", LETTERS[1:9]), str_c("10", LETTERS[1:10])) ~ "facility",
    violation_code %in% str_c("06", LETTERS[1:9]) ~ "hygiene",
    violation_code %in% str_c("04", LETTERS[11:15]) ~ "vermin",
    violation_code %in% c("07A", "99B") ~ "other_scored",
    !is.na(violation_code) ~ "not_scored"
  )
)
```

Here is a bar plot displaying the distribution
```{r}
restaurants_raw %>% 
  group_by(violation_group) %>% 
  filter(!is.na(violation_group)) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(reorder(violation_group, -n), n)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle=60, hjust=1))
```

Some categories barely contain any observations. For the time being, we will keep the chosen binning. We are going to revisit this point in the context of feature engineering.

Here is the distribution of violation code again. This time we incorporate the variable violation group.

```{r}
restaurants_raw %>% 
  filter(!is.na(violation_group)) %>% 
  ggplot(aes(x = violation_code, fill = violation_group)) +
  geom_bar() +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

For later use, we construct indicator variables for the different violation groups:

```{r}
for(level in unique(restaurants_raw$violation_group)){
  if (is.na(level)) {
    next
  } else {
    restaurants_raw[paste("viol", level, sep = "_")] <- 
    ifelse(restaurants_raw$violation_group == level, 1, 0)
  }
}
names(restaurants_raw)
```



### 2.6 Inspection types

The variable inspection type takes on 34 distinct values:

```{r}
restaurants_raw %>% 
     summarise(n_distinct(inspection_type))
```

These are the values:

```{r}
restaurants_raw %>% 
  group_by(inspection_type) %>% 
  summarise()
```

We collapse these values into a smaller range:

```{r}
restaurants_raw$inspection_type2 <-  restaurants_raw %>% 
  select(inspection_type) %>% 
  separate(inspection_type, into = c("before_slash", "after_slash"), sep = " / ") %>% 
  transmute(after_slash = str_replace_all(after_slash, fixed(" "), fixed(""))) %>% 
  transmute(inspection_type2 = str_replace_all(after_slash, fixed("-i"), fixed("I"))) %>% 
  .$inspection_type2

unique(restaurants_raw$inspection_type2)
```

Here a display of the distribution:

```{r}
restaurants_raw %>% 
  group_by(inspection_type2) %>% 
  summarise(n = n()) %>% 
  ggplot(aes(reorder(as.factor(inspection_type2), -n), n)) +
  geom_bar(stat = "identity", color = "orange") +
  xlab("Inspection type") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Again, there a are categories that are 'almost empty'. The vast majority of observations are initial and re-inspections:

```{r}
restaurants_raw %>% 
  group_by(inspection_type2) %>% 
  summarise(n = n(), prop = round(n/nrow(restaurants_raw), 5)) %>% 
  arrange(desc(n))
```

For later purposes, we contruct a dummy variable for the category 'InitialInspection'.

```{r}
restaurants_raw$dummy_InitialInspection <- 
  as.factor(ifelse(restaurants_raw$inspection_type2 == "InitialInspection", "Initial", "Not_Initial"))
names(restaurants_raw)
```


### 2.7 Grade

```{r}
restaurants_raw %>% 
  group_by(grade) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n))
```

There is a massive amount of NA's. We are going to investigate how the number of NAs for ‘grade’ relates to ‘score’ and ‘inspection_type.’ More precisely, we know that on initial inspections no grade is issued, if the corresponding score is above 14. It seems plausible that this would lead to a value of NA for grade.

```{r}
restaurants_raw %>% 
  select(inspection_type2, score, grade) %>% 
  filter(inspection_type2 == "InitialInspection", score > 14) %>%
  group_by(grade) %>% 
  summarise(n = n())
```

So about 73 % of the missing values are due to a score of 14 or higher. Furthermore, out of the 1915 observations with value ‘Not yet graded’, 1548 are the result of a high score (above 13) in the initial inspection. We will recode these NA’s to “Not Yet Graded” to reduce the number of missing values for ‘grade’.

```{r}
# Recode NA's with inspection type = 'InitialInspection' and 'score' > 14 to "Not Yet Graded":
restaurants_raw <- restaurants_raw %>%
  mutate(grade = case_when(
    is.na(.$grade) &
      .$inspection_type2 == "InitialInspection" &
      .$score > 14 ~ "Not Yet Graded",
    TRUE ~ grade
  ))
```

We have reduced the number of NA's significantly:

```{r}
restaurants_raw %>% 
  summarise(prop_na = round(mean(is.na(grade)), 2),
            n_not_graded = sum(grade == "Not Yet Graded", na.rm = T))
```

Here is the distribution of the proportion of inspections with an NA for grade per restaurant:

```{r}
restaurants_raw %>% 
  group_by(id, inspection_date) %>% 
  summarise(prop_gradeNA = mean(is.na(grade))) %>% 
  ggplot(aes(x = prop_gradeNA)) + 
  geom_histogram(bins = 30) + 
  xlab("proportion of NA's for inspection grade per restaurant")
```

It seems that for most restaurants there is no inspection with a missing value for  On the other hand, there are some restaurants for whom all values for grade are NA.

```{r}
restaurants_raw %>% 
  group_by(id, inspection_date) %>% 
  summarise(prop_gradeNA = round(mean(is.na(grade)), 3)) %>% 
  ungroup() %>% 
  group_by(prop_gradeNA) %>% 
  summarise(proportion_of_restaurants = round(n()/nrow(.), 3)) %>% 
  arrange(desc(proportion_of_restaurants))
```


As we did before, we are going to construct dummy variables, this time for the different levels of grade. We'll use "Z" as base level: 

```{r}
temp_index <- restaurants_raw$grade == "Not Yet Graded"
restaurants_raw$grade[temp_index] <- "NotYetGraded"
rm(temp_index)
for(level in unique(restaurants_raw$grade)){
  
  if (is.na(level) | level == "Z") {
    next
  } else {
    restaurants_raw[paste("grade", level, sep = "_")] <- 
      ifelse(restaurants_raw$grade == level, 1, 0)
  }
  
}

rm(level)
names(restaurants_raw)
```

```{r}
restaurants_raw <- restaurants_raw %>% 
  mutate(grade = as.factor(grade))
```


### 2.8 Score

Let us turn to 'score'.

```{r}
summary(restaurants_raw$score)
```



There are 19514 NA’s. So let us look at the proportion of inspections that exhibit an NA for 'score'. 

```{r}
restaurants_raw %>% 
  group_by(id, inspection_date) %>% 
  summarise(n_na = sum(is.na(score))) %>% 
  ungroup() %>% 
  summarise(n_na = round(sum(n_na)/n(), 3))
```

15 % of all inspections have a missing value for 'score'.


We know that there are 16530 observations that refer to violations that are not scored:

```{r}
restaurants_raw %>% 
  group_by(violation_group) %>% 
  summarise(n = n()) %>% 
  filter(violation_group == "not_scored")    
```


These observations are expected to have NA for ‘score’:
```{r}
restaurants_raw %>% 
  select(violation_group, score) %>% 
  filter(violation_group == "not_scored") %>% 
  summarise(n = n(), n_na_score = sum(is.na(score)), prop_na_score = round(n_na_score/n, 2))
```

Hence, these observations account for 82 % of all of the 19514 NA’s for ‘score’. What about the remaining 3428 missing values for ‘score’? Let us consider the relationship with violation group:

```{r}
restaurants_raw %>%
  select(violation_group, score) %>%
  filter(is.na(violation_group)) %>%
  summarise(n = n(), n_na_score = sum(is.na(score)))
```

Hence, the rest of the NA’s for score are associated with NA’s on ‘violation_group’. So let us turn to the NA’s for the variable ‘violation_group’. One cause for an NA here may be that on a given inspection, there was no violation at all. So what is the number of NA's for violation group among observations that do not exhibit a violation?

```{r}
restaurants_raw %>% filter(action == "NoViol") %>% 
  summarise(viol_group_NA = sum(is.na(violation_group)))
```



## 3. Predicting the number of days until the next inspection


### 3.1 Split into training and test data

As a first step, we will divide our data set into a training and a test data set, putting 2/3 of the restaurants into the training set.

```{r}
set.seed(1)
train_ids <- sample(unique(restaurants_raw$id), 0.6*length(unique(restaurants_raw$id)))
test_ids <- setdiff(unique(restaurants_raw$id), train_ids) 
```

Next, let us see if the distributions of the target feature in training and test data are similar:

```{r}
restaurants_raw %>% 
  select(id, inspection_date) %>% 
  group_by(id, inspection_date) %>% 
  arrange(inspection_date) %>% 
  summarise() %>% 
  mutate(difference = lead(inspection_date) - inspection_date) %>%
  filter(!is.na(difference)) %>% 
  mutate(group = ifelse(id %in% train_ids, "Training", "Test")) %>% 
  ggplot(aes(as.numeric(difference))) +
  geom_histogram(bins = 30) +
  scale_x_continuous(breaks = seq(0, 1000, by = 100)) +
  xlab("Days until next inspection") +
  facet_grid(. ~ group) 
```

The distribution of the target seems reasonably similar across training and test data. Now, we'll split the data set into training and test data:


```{r}
# Training data
train_df <- restaurants_raw %>% 
  filter(id %in% train_ids)
# Test data
test_df <- restaurants_raw %>% 
  filter(id %in% test_ids) 
```


So now, we remove the original data set and test data from the workspace and save the latter to a file:

```{r}
write.csv(test_df, "test_restaurants.csv")
rm(restaurants_raw)
```


### 3.2 Features and response

In the following we will use regression trees to predict the number of days until the next inspection, using the score and grade on the last inspection, and number of violations in the respective violation groups on the last inspection.

```{r}
## Create data set with features for inspection type, score, grade and number of violations:

make_df_InitScoreGradeDaysCuisine <- function(df) {
  df %>% 
    select(id, inspection_date, dummy_InitialInspection, score, 
           grade, cuisine_descr) %>% 
    group_by(id, inspection_date) %>% 
    arrange(id, inspection_date) %>% 
    summarise_at(vars(c("grade","dummy_InitialInspection", "score", "cuisine_descr")), 
                 .funs = list(first)) %>% 
    mutate(days_until_next = lead(inspection_date) - inspection_date) %>%
    ungroup()
}

make_df_sumViolFlags <- function(df) {
  df %>% 
    select(id, inspection_date, starts_with("viol_"), starts_with("critical")) %>% 
  mutate(critical_flag = case_when( critical_flag == "Critical" ~ 1, 
                                    TRUE ~ 0 )) %>% 
  group_by(id, inspection_date) %>% 
  arrange(id, inspection_date) %>% 
  summarise_at(vars(starts_with("viol"), starts_with("critical")), ~ sum(., na.rm = TRUE)) %>% 
  ungroup()
}

make_df <- function(df) {
    left_join( make_df_InitScoreGradeDaysCuisine(df) , make_df_sumViolFlags(df) ,
              by = c("id", "inspection_date") ) %>% 
    filter(!is.na(days_until_next)) %>% 
    select(id, inspection_date, days_until_next, score, everything())
}
```




### 3.3 First Model

```{r}
library(rpart)
library(rpart.plot)
```


The first model will leave the NA's for 'score' as is. 


```{r}
(m1 <- rpart(days_until_next ~ ., 
             data = train_df %>% make_df() %>% select(-id, -inspection_date), 
             method = "anova" ))
```


```{r}
rpart.plot(m1, type = 3, digits = 3, fallen.leaves = TRUE)
```


### 3.4 Second Model: Imputed NA's

So far, so good. Let us now impute the missing values for score and grade by predicting those values using observations on all other variables where they are not missing. 

First, we'll recombine training and test data:

```{r}
combi <- train_df %>% 
  make_df() %>% 
  bind_rows(make_df(test_df))
```

Then we impute the missing values for score and grade by fitting regression trees on the respective sets of non-missing observations: 

```{r}
map ( combi %>% select( -id, -inspection_date ), ~ sum ( is.na ( . ) ) )
```


```{r}
# Score
scoreFit <- combi %>% filter(!is.na(score)) %>% select(-id, -inspection_date, -days_until_next) %>%
  rpart(score ~ ., data = ., method = "anova")
combi$score[is.na(combi$score)] <- predict(scoreFit, combi[is.na(combi$score),])

# Days_since_last
# daysFit <- combi %>% filter(!is.na(days_since_last)) %>% select(-id, -inspection_date, -days_until_next) %>%   rpart(days_since_last ~ ., data = ., method = "anova")
# combi$days_since_last[is.na(combi$days_since_last)] <-
#   predict(daysFit, combi[is.na(combi$days_since_last),])

# Grade
gradeFit <- combi %>% filter(!is.na(grade)) %>% select(-id, -inspection_date, -days_until_next) %>%
  rpart(grade ~ ., data = ., method = "class")
combi$grade[is.na(combi$grade)] <- predict(gradeFit, combi[is.na(combi$grade),], type = "class")

```

Check that there are no missing values any more:

```{r}
map(combi %>% select(score, grade), ~ sum(is.na(.)))
```

Finally we split the data into training and test data again:

```{r}
train_df <- combi %>% 
  filter(id %in% train_ids) %>% 
  select(-id, -inspection_date)

test_df <- combi %>%
  filter(id %in% test_ids) %>% 
  select(-id, -inspection_date)
```


So next, we'll fit a second model with imputed NA's:

```{r}
(m2 <- rpart(days_until_next ~ .,
             data = train_df,
             method = "anova"))
```

```{r}
rpart.plot(m2, type = 3, digits = 3, fallen.leaves = TRUE)
```

### 3.5 Cross Validation

Finally let's use cross validation to fit one final model:

```{r}
library(caret)
library(e1071)
```


```{r}
set.seed(42)
myControl <- trainControl(
  
  method = "cv",
  number = 10
  
)

cp.grid <- expand.grid(.cp = (1:10)*0.01)
```


```{r}
set.seed(42)
(m3 <- train(as.numeric(days_until_next) ~ ., 
            data = train_df, 
            method = "rpart", 
            trControl = myControl, 
            tuneGrid = cp.grid
             )
 )
```


```{r}
rpart.plot(m3$finalModel, type = 3, digits = 3, fallen.leaves = TRUE)
```

### 3.4 Linear Regression Model with all predictors

```{r}
(m4 <- train(
  as.numeric(days_until_next) ~ .,
  data = train_df,
  method = "glm"))
```


### 3.5 Discretizing the Response based on the three modes in distribution of 'days_until_next'

```{r}
train_df %>% 
ggplot(aes(x = as.numeric(days_until_next))) + 
  geom_density() +
  scale_x_continuous(breaks = seq(0, 900, 50)) +
  xlab("days until next inspection")
```

We will bin the response variable according to the 3 humps in the distribution. The boundaries will be 100 and 300. We will train a decision tree using 10-fold cross validation:

```{r}
set.seed(3333)

(m5 <- train_df %>% mutate(days_until_next = case_when(
  between(as.numeric(days_until_next), 0, 100) ~ "within the next 2 to 3 months",
  between(as.numeric(days_until_next), 101, 300) ~ "within the next 10 months",
  TRUE ~ "in more than 10 months"),
  days_until_next = as.factor(days_until_next)) %>% 
    train(days_until_next ~ ., 
          data = ., 
          method = "rpart", 
          trControl = myControl, 
          tuneGrid = cp.grid)
)
```

```{r}
rpart.plot(m5$finalModel, type = 3, digits = 3, fallen.leaves = TRUE)
```

Let  us try a different binning of the target: 

```{r}
set.seed(3333)

cp.grid <- expand.grid(.cp = (1:50)*0.01)

(m6 <- train_df %>% mutate(days_until_next = case_when(
  between(as.numeric(days_until_next), 0, 40) ~ "within next 40 days",
  between(as.numeric(days_until_next), 41, 190) ~ "within next 6 months",
  between(as.numeric(days_until_next), 191, 320) ~ "within next 6 to 10 months",
  TRUE ~ "in more than 10 months"),
  days_until_next = as.factor(days_until_next)) %>% 
    train(days_until_next ~ ., 
          data = ., 
          method = "rpart", 
          trControl = myControl, 
          tuneGrid = cp.grid)
)
```




